{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Module to train a graph convolutional network (GCN) model on a given graph dataset.\n",
    "\n",
    "\"\"\"\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "from typing import Any, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import from_networkx\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.backend.gcn import GCN\n",
    "from src.backend.generate_er_graphs import CVEGraphGenerator\n",
    "from src.backend.utils.graph_utils import split_edges_and_sample_negatives\n",
    "\n",
    "log_format = \"%(asctime)s - %(levelname)s - %(message)s\"\n",
    "logging.basicConfig(level=logging.INFO, format=log_format)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "\n",
    "def compute_metrics(labels, predictions, loss):\n",
    "    labels = torch.clamp(torch.round(labels), 0, 1).bool()\n",
    "    preds = predictions.bool()\n",
    "    # True Positives, False Positives, and False Negatives\n",
    "    tp = torch.sum(preds & labels).item()\n",
    "    fp = torch.sum(preds & ~labels).item()\n",
    "    fn = torch.sum(~preds & labels).item()\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = torch.sum(preds == labels).item() / labels.numel()\n",
    "\n",
    "    # Precision, Recall, and F1 Score\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return loss, accuracy, precision, recall, f1\n",
    "\n",
    "\n",
    "def train_epoch(\n",
    "    model: torch.nn.Module,\n",
    "    loader,  # Using DataLoader instead of a single Data object\n",
    "    train_data,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    criterion: Any,\n",
    "    device: torch.device,\n",
    ") -> Tuple[float, Tuple[float, float, float, float, float, str], torch.Tensor, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Train the model for one epoch with batch processing.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The graph convolutional network (GCN) model to be trained.\n",
    "        loader (DataLoader): DataLoader providing batches of graph data.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer to be used for training.\n",
    "        criterion (torch.nn.modules.loss._Loss): The loss function used for training.\n",
    "        device (torch.device): The device (CPU or CUDA) on which the model is being trained.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, Tuple[float, float, float, float, float, str], Tensor, np.ndarray, np.ndarray]:\n",
    "        A tuple containing the average loss for the epoch, a tuple of various evaluation metrics,\n",
    "        the model logits, binary predictions, and labels for the last batch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_logits = []\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    for batch_data in loader:\n",
    "        print(batch_data)\n",
    "        batch_data = collate(batch_data, train_data)\n",
    "        print(batch_data)\n",
    "        # batch_data = collate(batch_data)\n",
    "        z = model(batch_data.x.to(device), batch_data.edge_index.to(device))\n",
    "        logits = model.decode(z, batch_data.train_pos_edge_index.to(device), batch_data.train_neg_edge_index.to(device))\n",
    "        labels = torch.cat(\n",
    "            [torch.ones(batch_data.train_pos_edge_index.size(1)), torch.zeros(batch_data.train_neg_edge_index.size(1))], dim=0\n",
    "        ).to(device)\n",
    "\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        all_logits.append(logits.detach())\n",
    "        all_predictions.append((torch.sigmoid(logits) > 0.5).detach())\n",
    "        all_labels.append(labels.detach())\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    metrics = compute_metrics(torch.cat(all_labels), torch.cat(all_predictions), avg_loss)\n",
    "    torch.cuda.empty_cache()\n",
    "    return loss.item(), metrics, all_logits, all_predictions, all_labels\n",
    "\n",
    "\n",
    "def eval_epoch(\n",
    "    model: torch.nn.Module, data: Data, criterion: Any, device: torch.device\n",
    ") -> Tuple[float, Tuple[float, float, float, float, float, str], torch.Tensor, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Evaluate the model on validation or test data for one epoch.\n",
    "\n",
    "    Args:\n",
    "        model (Module): The graph convolutional network (GCN) model to be evaluated.\n",
    "        data (Data): The data object from torch_geometric containing\n",
    "            graph data including node features and edge indices for validation or testing.\n",
    "        criterion (Any): The loss function used for evaluation.\n",
    "        device (torch.device): The device (CPU or CUDA) on which the model is being evaluated.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, Tuple[float, float, float, float, float, str], Tensor, np.ndarray, np.ndarray]:\n",
    "        A tuple containing the loss for the epoch,\n",
    "        a tuple of various evaluation metrics (accuracy, precision, recall,\n",
    "        F1 score, ROC-AUC score, classification report),\n",
    "        the model logits, binary predictions, and labels.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z = model(data.x.to(device), data.edge_index.to(device))\n",
    "        logits = model.decode(z, data.val_pos_edge_index.to(device), data.val_neg_edge_index.to(device))\n",
    "        labels = torch.cat(\n",
    "            [torch.ones(data.val_pos_edge_index.size(1)), torch.zeros(data.val_neg_edge_index.size(1))], dim=0\n",
    "        ).to(device)\n",
    "\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        predictions = torch.sigmoid(logits) > 0.5\n",
    "\n",
    "        metrics = compute_metrics(labels, predictions, loss)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    return loss.item(), metrics, logits, predictions, labels\n",
    "\n",
    "\n",
    "def prepare_data(graph_save_path: str, vectorizer_path: str) -> Tuple[Any, int]:\n",
    "    \"\"\"\n",
    "    Prepares graph data for the GCN model from a given file path.\n",
    "\n",
    "    Args:\n",
    "        graph_save_path (str): The path to the saved graph.\n",
    "        features_path (str): The path to the saved features.\n",
    "        vectorizer_path (str): The path to the saved vectorizer.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Any, int]: A tuple containing the prepared data and the number of features.\n",
    "                        The prepared data is a torch_geometric Data object with node features and edge indices.\n",
    "                        The number of features is an integer representing the size of the feature vector for each node.\n",
    "    \"\"\"\n",
    "    logging.info(\"Loading graph data...\")\n",
    "    generator = CVEGraphGenerator(file_path=\"\")\n",
    "    generator.load_graph(graph_save_path, vectorizer_path)\n",
    "    graph = generator.graph\n",
    "    num_features = generator.ft_model.get_dimension()\n",
    "    logging.info(\"Number of features: %d\", num_features)\n",
    "\n",
    "    data = from_networkx(graph)\n",
    "    logging.info(\"nx graph transformed to torch_geometric data object\")\n",
    "    node_features = [graph.nodes[node][\"vector\"] for node in graph.nodes()]\n",
    "\n",
    "    data.x = torch.tensor(node_features, dtype=torch.float)\n",
    "\n",
    "    return data, num_features\n",
    "\n",
    "\n",
    "def save_checkpoint(state, filename=\"checkpoint.pth.tar\"):\n",
    "    \"\"\"\n",
    "    Save the training model at the checkpoint.\n",
    "\n",
    "    Args:\n",
    "        state (dict): Contains model's state_dict, optimizer's state_dict, epoch, etc.\n",
    "        filename (str): Name of the checkpoint file.\n",
    "    \"\"\"\n",
    "    torch.save(state, filename)\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint_path, model, optimizer):\n",
    "    \"\"\"\n",
    "    Load a model checkpoint.\n",
    "\n",
    "    Args:\n",
    "        checkpoint_path (str): Path to the checkpoint file.\n",
    "        model (torch.nn.Module): The model to load the checkpoint into.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer to load the checkpoint into.\n",
    "\n",
    "    Returns:\n",
    "        int: The epoch to resume training from.\n",
    "        float: The best validation loss recorded up to this checkpoint.\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "    return checkpoint.get(\"epoch\", 0), checkpoint.get(\"best_val_loss\", float(\"inf\"))\n",
    "\n",
    "\n",
    "def _plot_results(metrics):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(metrics[\"train\"][\"loss\"], label=\"Training Loss\")\n",
    "    plt.plot(metrics[\"val\"][\"loss\"], label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training and Validation Loss Over Epochs\")\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import logging\n",
    "from typing import Tuple\n",
    "\n",
    "from torch import randperm\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "log_format = \"%(asctime)s - %(levelname)s - %(message)s\"\n",
    "logging.basicConfig(level=logging.INFO, format=log_format)\n",
    "\n",
    "# https://chat.openai.com/share/e9495e15-6218-4cfa-b4e2-b10466a807ff\n",
    "#  need to have custom dataloaders here for batch training\n",
    "\n",
    "\n",
    "def split_edges_and_sample_negatives(data: Data, train_perc: float, valid_perc: float) -> Tuple[Data, Data, Data]:\n",
    "    \"\"\"\n",
    "    Splits edges of the graph into training, validation, and test sets and performs negative sampling.\n",
    "\n",
    "    Args:\n",
    "    data (Data): A torch_geometric Data object containing the graph data, including edge indices.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[Data, Data, Data]: A tuple containing Data objects for training, validation, and testing.\n",
    "                              Each Data object includes features (x), edge indices (edge_index),\n",
    "                              positive edges for training/validation/testing (train_pos_edge_index,\n",
    "                              val_pos_edge_index, test_pos_edge_index) and negative edges\n",
    "                              (train_neg_edge_index, val_neg_edge_index, test_neg_edge_index).\n",
    "    \"\"\"\n",
    "    num_edges = data.edge_index.size(1)\n",
    "    num_nodes = data.num_nodes\n",
    "\n",
    "    # Shuffle and split edges\n",
    "    perm = randperm(num_edges)\n",
    "    num_train = int(num_edges * train_perc)  # 80% for training\n",
    "    num_val = int(num_edges * valid_perc)  # 10% for validation\n",
    "\n",
    "    train_edge = data.edge_index[:, perm[:num_train]]\n",
    "    val_edge = data.edge_index[:, perm[num_train : num_train + num_val]]\n",
    "    test_edge = data.edge_index[:, perm[num_train + num_val :]]\n",
    "\n",
    "    # Negative sampling\n",
    "    train_edge_neg = negative_sampling(\n",
    "        edge_index=data.edge_index, num_nodes=num_nodes, num_neg_samples=train_edge.size(1)\n",
    "    )\n",
    "    val_edge_neg = negative_sampling(edge_index=data.edge_index, num_nodes=num_nodes, num_neg_samples=val_edge.size(1))\n",
    "    test_edge_neg = negative_sampling(\n",
    "        edge_index=data.edge_index, num_nodes=num_nodes, num_neg_samples=test_edge.size(1)\n",
    "    )\n",
    "\n",
    "    # Creating Data objects for training, validation, and testing\n",
    "    train_data = Data(\n",
    "        x=data.x, edge_index=train_edge, train_pos_edge_index=train_edge, train_neg_edge_index=train_edge_neg\n",
    "    )\n",
    "    val_data = Data(x=data.x, edge_index=val_edge, val_pos_edge_index=val_edge, val_neg_edge_index=val_edge_neg)\n",
    "    test_data = Data(x=data.x, edge_index=test_edge, test_pos_edge_index=test_edge, test_neg_edge_index=test_edge_neg)\n",
    "\n",
    "    return train_data, val_data, test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch_geometric.data import Batch\n",
    "\n",
    "from torch_geometric.data import DataLoader, Batch\n",
    "\n",
    "class EdgeBatchLoader(DataLoader):\n",
    "    def __init__(self, data, edge_index, batch_size=32, **kwargs):\n",
    "        self.data = data\n",
    "        self.edge_index = edge_index\n",
    "        self.batch_size = batch_size\n",
    "        super().__init__(range(edge_index.size(1)), batch_size=batch_size, **kwargs)\n",
    "\n",
    "    def collate(self, batch_indices):\n",
    "        # Select edges for the current batch\n",
    "        batch_edges = self.edge_index[:, batch_indices]\n",
    "\n",
    "        # Extract the node features for the nodes involved in these edges\n",
    "        node_indices = torch.unique(batch_edges)\n",
    "        batch_x = self.data.x[node_indices]\n",
    "\n",
    "        # Create a Data object for this batch\n",
    "        batch_data = Data(x=batch_x, edge_index=batch_edges)\n",
    "        return batch_data\n",
    "    \n",
    "def collate(batch_indices, data):\n",
    "    # Select edges for the current batch\n",
    "    batch_edges = data.edge_index[:, batch_indices]\n",
    "\n",
    "    # Extract unique nodes from these edges\n",
    "    unique_nodes = torch.unique(batch_edges)\n",
    "\n",
    "    # Extract the node features for the nodes involved in these edges\n",
    "    batch_x = data.x[unique_nodes]\n",
    "\n",
    "    # Create a new edge index for the batch, reindexed to account for the subset of nodes\n",
    "    batch_edge_index = torch.cat([\n",
    "        torch.where(unique_nodes[:, None] == batch_edges[0])[1],\n",
    "        torch.where(unique_nodes[:, None] == batch_edges[1])[1]\n",
    "    ], dim=0).reshape(2, -1)\n",
    "\n",
    "    # Create a Data object for this batch\n",
    "    batch_data = Data(x=batch_x, edge_index=batch_edge_index)\n",
    "    return batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    read_dir: str,\n",
    "    data_size: int,\n",
    "    hidden_dims: list,\n",
    "    logging_interval: int = 100,\n",
    "    checkpoint_path: str = \"models/checkpoint.pth.tar\",\n",
    "    load_from_checkpoint: bool = False,\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"\n",
    "    main logic to grab data, train model, and plot results\n",
    "\n",
    "    Args:\n",
    "        read_dir (str): The directory containing the files to read.\n",
    "        hidden_dims (list): The list of hidden dimensions for each layer.\n",
    "        logging_interval (int): The interval at which to log metrics.\n",
    "        checkpoint_path (str): The path to save the model checkpoint.\n",
    "        load_from_checkpoint (bool): Whether to load the best model checkpoint.\n",
    "\n",
    "    kwargs:\n",
    "        train_percent (float): The percentage of data to use for training.\n",
    "        valid_percent (float): The percentage of data to use for validation.\n",
    "        num_epochs (int): The number of training epochs.\n",
    "        learning_rate (float): The learning rate for the optimizer.\n",
    "        weight_decay (float): The weight decay for the optimizer.\n",
    "        dropout_rate (float): The dropout rate for the model.\n",
    "        plot_results (bool): Whether to plot the training and validation loss over epochs.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    train_percent = kwargs.get(\"train_percent\", 0.80)\n",
    "    valid_percent = kwargs.get(\"valid_percent\", 0.20)\n",
    "    num_epochs = kwargs.get(\"num_epochs\", 100)\n",
    "    learning_rate = kwargs.get(\"learning_rate\", 0.01)\n",
    "    weight_decay = kwargs.get(\"weight_decay\", 1e-5)\n",
    "    dropout_rate = kwargs.get(\"dropout_rate\", 0.5)\n",
    "    plot_results = kwargs.get(\"plot_results\", True)\n",
    "\n",
    "    num_features = 300  # hard coded for now\n",
    "    model = GCN(num_features=num_features, hidden_dims=hidden_dims, dropout_rate=dropout_rate).to(device)\n",
    "    logging.info(f\"model loaded onto device: {device}\")\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    if load_from_checkpoint:\n",
    "        start_epoch, best_val_loss = load_checkpoint(checkpoint_path, model, optimizer)\n",
    "        logging.info(f\"Loaded checkpoint at epoch {start_epoch} with best validation loss of {best_val_loss}\")\n",
    "        return None\n",
    "\n",
    "    logging.info(\"Training GCN model...\")\n",
    "\n",
    "    # Prepare data\n",
    "    # Construct paths dynamically based on the limit and output directory\n",
    "    base_path = os.path.join(read_dir, f\"samplesize_{data_size}\" if data_size else \"all_data\")\n",
    "    graph_save_path = os.path.join(base_path, \"graph.gml\")\n",
    "    vectorizer_path = os.path.join(base_path, \"ft_model.bin\")\n",
    "\n",
    "    data, num_features = prepare_data(graph_save_path, vectorizer_path)\n",
    "    train_data, val_data, _ = split_edges_and_sample_negatives(data, train_perc=train_percent, valid_perc=valid_percent)\n",
    "    BATCH_SIZE = 32\n",
    "    train_loader = EdgeBatchLoader(data=train_data, edge_index=train_data.edge_index, batch_size=BATCH_SIZE)\n",
    "    val_loader = EdgeBatchLoader(data=val_data, edge_index=val_data.edge_index, batch_size=BATCH_SIZE)\n",
    "\n",
    "    metric_keys = [\"loss\", \"accuracy\", \"precision\", \"recall\", \"f1\"]\n",
    "    metrics = {phase: {key: [] for key in metric_keys} for phase in [\"train\", \"val\"]}\n",
    "\n",
    "    best_val_loss = float(\"inf\")  # Initialize best validation loss for checkpointing\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        try:\n",
    "            torch.cuda.synchronize(device)\n",
    "            train_metrics = train_epoch(model, train_loader, train_data, optimizer, criterion, device)\n",
    "            val_metrics = eval_epoch(model, val_loader, criterion, device)\n",
    "\n",
    "            epoch_metrics = {\n",
    "                \"train\": train_metrics,  # Directly use the tuple\n",
    "                \"val\": val_metrics,  # Directly use the tuple\n",
    "            }\n",
    "\n",
    "            for phase in [\"train\", \"val\"]:\n",
    "                for key, value in zip(metric_keys, epoch_metrics[phase]):\n",
    "                    metrics[phase][key].append(value)\n",
    "\n",
    "            # Update the formatting to include metric names\n",
    "            train_metrics_formatted = \" - \".join(\n",
    "                f\"{name}: {metric:.4f}\" for name, metric in zip(metric_keys, train_metrics[1])\n",
    "            )\n",
    "            val_metrics_formatted = \" - \".join(\n",
    "                f\"{name}: {metric:.4f}\" for name, metric in zip(metric_keys, val_metrics[1])\n",
    "            )\n",
    "            if epoch % logging_interval == 0:\n",
    "                logging.info(f\"Epoch {epoch+1}:\")\n",
    "                logging.info(f\"Train Loss: {train_metrics[0]:.4f} - Val Loss: {val_metrics[0]:.4f}\")\n",
    "                logging.info(f\"Train Metrics: {train_metrics_formatted}\")\n",
    "                logging.info(f\"Val Metrics: {val_metrics_formatted}\")\n",
    "                # validation confusion matrix\n",
    "                # using sklearn\n",
    "                # from sklearn.metrics import confusion_matrix\n",
    "\n",
    "            # Checkpointing logic\n",
    "            is_best = val_metrics[0] < best_val_loss\n",
    "            if is_best:\n",
    "                best_val_loss = val_metrics[0]\n",
    "                logging.info(\n",
    "                    \"checkpointing model at epoch %d with best validation loss of %.3f\", epoch + 1, best_val_loss\n",
    "                )\n",
    "\n",
    "                save_checkpoint(\n",
    "                    {\n",
    "                        \"epoch\": epoch + 1,\n",
    "                        \"state_dict\": model.state_dict(),\n",
    "                        \"best_val_loss\": best_val_loss,\n",
    "                        \"optimizer\": optimizer.state_dict(),\n",
    "                    },\n",
    "                    filename=checkpoint_path,\n",
    "                )\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            torch.cuda.synchronize(device)\n",
    "            print(f\"Runtime error during epoch {epoch+1}: {e}\")\n",
    "            break\n",
    "\n",
    "    if plot_results:\n",
    "        _plot_results(metrics)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-22 16:27:08,696 - INFO - model loaded onto device: cuda\n",
      "2023-12-22 16:27:08,696 - INFO - Training GCN model...\n",
      "2023-12-22 16:27:08,696 - INFO - Loading graph data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "[nltk_data] Downloading package wordnet to /home/jdineen/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "2023-12-22 16:27:11,678 - INFO - Loading graph structure from data/samplesize_102/graph.gml...\n",
      "2023-12-22 16:27:11,687 - INFO - Loading vectorizer from data/samplesize_102/ft_model.bin...\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK resource 'stopwords' already downloaded.\n",
      "Downloading NLTK resource: wordnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-22 16:27:14,805 - INFO - Number of features: 300\n",
      "2023-12-22 16:27:14,818 - INFO - nx graph transformed to torch_geometric data object\n",
      "  0%|          | 0/100 [00:10<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'GlobalStorage' object has no attribute 'train_pos_edge_index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_data, val_data, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mread_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m102\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[57], line 71\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(read_dir, data_size, hidden_dims, logging_interval, checkpoint_path, load_from_checkpoint, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39msynchronize(device)\n\u001b[0;32m---> 71\u001b[0m     train_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m     val_metrics \u001b[38;5;241m=\u001b[39m eval_epoch(model, val_loader, criterion, device)\n\u001b[1;32m     74\u001b[0m     epoch_metrics \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m: train_metrics,  \u001b[38;5;66;03m# Directly use the tuple\u001b[39;00m\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m: val_metrics,  \u001b[38;5;66;03m# Directly use the tuple\u001b[39;00m\n\u001b[1;32m     77\u001b[0m     }\n",
      "Cell \u001b[0;32mIn[52], line 81\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, loader, train_data, optimizer, criterion, device)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# batch_data = collate(batch_data)\u001b[39;00m\n\u001b[1;32m     80\u001b[0m z \u001b[38;5;241m=\u001b[39m model(batch_data\u001b[38;5;241m.\u001b[39mx\u001b[38;5;241m.\u001b[39mto(device), batch_data\u001b[38;5;241m.\u001b[39medge_index\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[0;32m---> 81\u001b[0m logits \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mdecode(z, \u001b[43mbatch_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_pos_edge_index\u001b[49m\u001b[38;5;241m.\u001b[39mto(device), batch_data\u001b[38;5;241m.\u001b[39mtrain_neg_edge_index\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m     82\u001b[0m labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(\n\u001b[1;32m     83\u001b[0m     [torch\u001b[38;5;241m.\u001b[39mones(batch_data\u001b[38;5;241m.\u001b[39mtrain_pos_edge_index\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)), torch\u001b[38;5;241m.\u001b[39mzeros(batch_data\u001b[38;5;241m.\u001b[39mtrain_neg_edge_index\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m))], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     84\u001b[0m )\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     86\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(logits, labels)\n",
      "File \u001b[0;32m~/miniconda3/envs/xploit1/lib/python3.10/site-packages/torch_geometric/data/data.py:482\u001b[0m, in \u001b[0;36mData.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_store\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    478\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object was created by an older version of PyG. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    479\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf this error occurred while loading an already existing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    480\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset, remove the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m directory in the dataset\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    481\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroot folder and try again.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 482\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_store\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/xploit1/lib/python3.10/site-packages/torch_geometric/data/storage.py:87\u001b[0m, in \u001b[0;36mBaseStorage.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[key]\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m---> 87\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m     88\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     89\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GlobalStorage' object has no attribute 'train_pos_edge_index'"
     ]
    }
   ],
   "source": [
    "train_data, val_data, _ = main(read_dir=\"data\", data_size=102, hidden_dims=[16, 16, 16, 16, 16, 16, 16, 16, 16, 16], num_epochs=100, learning_rate=0.01, weight_decay=1e-5, dropout_rate=0.5, plot_results=True)\n",
    "# BATCH_SIZE = 32\n",
    "# print(train_data)\n",
    "# edge_batch_loader = EdgeBatchLoader(data=train_data, edge_index=train_data.edge_index, batch_size=BATCH_SIZE)\n",
    "\n",
    "# # Example usage\n",
    "# for batch_indices in edge_batch_loader:\n",
    "#     batch_data = collate(batch_indices, train_data)\n",
    "#     print(batch_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[204, 300], edge_index=[2, 163], train_pos_edge_index=[2, 163], train_neg_edge_index=[2, 163], batch=[204], ptr=[2])\n"
     ]
    }
   ],
   "source": [
    "for i in train_loader:\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xploit123456",
   "language": "python",
   "name": "xploit123456"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
